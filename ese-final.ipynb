{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler,OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import (\n    accuracy_score,\n    log_loss,\n    precision_score,\n    recall_score,\n    f1_score,\n    classification_report,\n    confusion_matrix\n)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# ==========================================\n# GLOBAL WARNING CONTROL (SAFE & CLEAN)\n# ==========================================\nimport warnings\n\n# 1️⃣ Ignore known, harmless FutureWarnings (seaborn / pandas)\nwarnings.filterwarnings(\n    \"ignore\",\n    category=FutureWarning\n)\n\n# 2️⃣ Ignore pandas RuntimeWarnings from NaN comparisons\nwarnings.filterwarnings(\n    \"ignore\",\n    category=RuntimeWarning,\n    module=\"pandas\"\n)\n\n# 3️⃣ Ignore seaborn warnings (visualization only)\nwarnings.filterwarnings(\n    \"ignore\",\n    module=\"seaborn\"\n)\n\n# 4️⃣ Safety: ensure numpy doesn't spam invalid comparisons\nnp.seterr(invalid='ignore')\n\ntrain=pd.read_csv(\"/kaggle/input/final-everything/train.csv\")\ntest=pd.read_csv(\"/kaggle/input/final-everything/test.csv\")\n\ntrain.isnull().sum()\n\ntest.isnull().sum()\n\ntrain = train.dropna(subset=['fruit_name']) #output label\n\ntest_id=test['id']\ntest=test.drop(columns=['id'])\n\nX=train.drop(columns=['fruit_name'])\ny=train['fruit_name']\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42,stratify=y)\n\nnumeric_features=X.select_dtypes(include=['int64','float64']).columns\ncategorical_features=X.select_dtypes(include=['object','category']).columns\n\nnumerical_pipeline=Pipeline(steps=[\n    ('impute',SimpleImputer(strategy='mean')),\n    ('scaler',StandardScaler())\n])\ncategorical_pipeline=Pipeline(steps=[\n    ('impute',SimpleImputer(strategy='most_frequent')),\n    ('encode',OneHotEncoder(handle_unknown='ignore'))\n])\n\n#VISUALISAION\n# STEP 1: HISTPLOT\n# ==========================================\nprint(\"Step 1: Histplots\")\nfor col in numeric_features:\n    plt.figure(figsize=(8, 4))\n    sns.histplot(X[col].dropna(), kde=True, color='royalblue')\n    plt.show()\n\n# ==========================================\n# STEP 2: TARGET COUNTS\n# ==========================================\nprint(\"\\nStep 2: Target Counts\")\nprint(y.value_counts())\nsns.countplot(x=y)\nplt.show()\n\n\n# ==========================================\n# STEP 3: BOXPLOT (Before)\n# ==========================================\nprint(\"\\nStep 3: Boxplots (Before)\")\nfor col in numeric_features:\n    plt.figure(figsize=(8, 2))\n    sns.boxplot(x=X[col], color='tomato')\n    plt.show()\n\n\n# ==========================================\n# STEP 4: OUTLIER & INF HANDLING (ROBUST VERSION)\n# ==========================================\nprint(\"\\nStep 4: Handling Outliers & Converting Infinity to NaN\")\n\n# 1️⃣ Replace inf → NaN\nX_train[numeric_features] = X_train[numeric_features].replace([np.inf, -np.inf], np.nan)\nX_test[numeric_features]  = X_test[numeric_features].replace([np.inf, -np.inf], np.nan)\n\n# 2️⃣ Compute IQR on TRAIN\nQ1 = X_train[numeric_features].quantile(0.25)\nQ3 = X_train[numeric_features].quantile(0.75)\nIQR = Q3 - Q1\n\n# 3️⃣ Keep only valid columns (IQR > 0 and not NaN)\nvalid_cols = IQR[(IQR > 0) & (~IQR.isna())].index\n\n# 4️⃣ Clip only valid columns\nlower = Q1[valid_cols] - 1.5 * IQR[valid_cols]\nupper = Q3[valid_cols] + 1.5 * IQR[valid_cols]\n\nX_train[valid_cols] = X_train[valid_cols].clip(lower, upper, axis=1)\nX_test[valid_cols]  = X_test[valid_cols].clip(lower, upper, axis=1)\n\n\n# ==========================================\n# STEP 5: RE-CHECK TARGET COUNTS\n# ==========================================\nprint(f\"Total Unique Classes: {y.nunique()}\")\nprint(\"-\" * 30)\nprint(y.value_counts())\n\n# ==========================================\n# STEP 6: RE-CHECK BOXPLOTS (AFTER CLEANING)\n# ==========================================\nprint(\"\\nStep 6: Final Visual Checks\")\n\nfor col in numeric_features:\n    plt.figure(figsize=(8, 2))\n    sns.boxplot(x=X_train[col], color='limegreen')\n    plt.title(f\"Boxplot of {col}\")\n    plt.show()\n\n\n# ==========================================\n# STEP 7: NORMAL PAIRPLOT\n# ==========================================\nprint(\"\\nStep 7: Generating Normal Pairplot\")\n\nplot_df = pd.concat([X_train, y_train], axis=1).sample(\n    min(500, len(X_train)),\n    random_state=42\n)\n\nsns.pairplot(plot_df, hue='fruit_name')\nplt.show()\n\n# ==========================================\n# STEP 8: HEATMAP (Fixed)\n# ==========================================\nprint(\"\\nStep 8: Corrected Heatmap\")\nplt.figure(figsize=(10, 8))\nsns.heatmap(X.corr(numeric_only=True), annot=True, cmap='RdYlBu', center=0, square=True)\nplt.show()\n\n# from sklearn.linear_model import LogisticRegression\n# model = LogisticRegression(\n#     C=1.0,\n#     penalty='l2',\n#     solver='lbfgs',\n#     max_iter=1000,\n#     n_jobs=-1\n# )\n\n# from sklearn.ensemble import RandomForestClassifier\n# model = RandomForestClassifier(\n#     n_estimators=400,\n#     max_depth=None,\n#     min_samples_split=5,\n#     min_samples_leaf=2,\n#     max_features='sqrt',\n#     random_state=42,\n#     n_jobs=-1\n# )\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nmodel = GradientBoostingClassifier(\n    n_estimators=300,\n    learning_rate=0.05,\n    max_depth=4,\n    subsample=0.9,\n    min_samples_split=5,\n    min_samples_leaf=2,\n    random_state=42\n)\n\n# from sklearn.ensemble import AdaBoostClassifier\n# model = AdaBoostClassifier(\n#     n_estimators=300,\n#     learning_rate=0.05,\n#     random_state=42\n# )\n\n# from sklearn.ensemble import ExtraTreesClassifier\n# model = ExtraTreesClassifier(\n#     n_estimators=500,\n#     max_depth=None,\n#     min_samples_split=5,\n#     min_samples_leaf=2,\n#     max_features='sqrt',\n#     random_state=42,\n#     n_jobs=-1\n# )\n\n# from xgboost import XGBClassifier\n# model = XGBClassifier(\n#     n_estimators=600,\n#     learning_rate=0.05,\n#     max_depth=6,\n#     subsample=0.9,\n#     colsample_bytree=0.9,\n#     reg_lambda=1,\n#     objective='multi:softprob',\n#     eval_metric='mlogloss',\n#     tree_method='hist',\n#     random_state=42,\n#     n_jobs=-1\n# )\n\n# from lightgbm import LGBMClassifier\n# model = LGBMClassifier(\n#     n_estimators=600,\n#     learning_rate=0.05,\n#     max_depth=-1,\n#     num_leaves=63,\n#     subsample=0.9,\n#     colsample_bytree=0.9,\n#     random_state=42,\n#     n_jobs=-1\n# )\n\n# from catboost import CatBoostClassifier\n# model = CatBoostClassifier(\n#     iterations=600,\n#     learning_rate=0.05,\n#     depth=6,\n#     loss_function='MultiClass',\n#     random_seed=42,\n#     verbose=False\n# )\n\npreprocessing=ColumnTransformer(transformers=[\n    ('num',numerical_pipeline,numeric_features),\n    ('cat',categorical_pipeline,categorical_features)\n])\n\npipeline=Pipeline(steps=[\n    ('preprocessor',preprocessing),\n    ('model',model)\n])\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ny_train_enc = le.fit_transform(y_train)  # fit on train\ny_test_enc = le.transform(y_test)        # transform test\n\npipeline.fit(X_train,y_train_enc)\n\n# Predict class labels\ny_pred = pipeline.predict(X_test)\n# Predict class probabilities (needed for log-loss, AUC, calibration)\ny_pred_proba = pipeline.predict_proba(X_test)\n\n# Accuracy\nacc = accuracy_score(y_test_enc, y_pred)\n\n# Log Loss\nll = log_loss(y_test_enc, y_pred_proba)\n\n# Precision, Recall, F1 (weighted = handles class imbalance)\nprec = precision_score(y_test_enc, y_pred, average='weighted')\nrec  = recall_score(y_test_enc, y_pred, average='weighted')\nf1   = f1_score(y_test_enc, y_pred, average='weighted')\n\nprint(\"Accuracy :\", acc)\nprint(\"Log Loss :\", ll)\nprint(\"Precision:\", prec)\nprint(\"Recall   :\", rec)\nprint(\"F1 Score :\", f1)\n\nfinal_preds=pipeline.predict(test)\nfinal_probs=pipeline.predict_proba(test)\n\n# ==========================================\n# STEP 10: FINAL DATA PREPARATION\n# (NO ROW DROPPED, NO NaNs INTRODUCED)\n# ==========================================\nprint(\"\\nStep 10: Preparing Final Submission DataFrames...\")\n# Decode predicted labels\ndecoded_labels = le.inverse_transform(final_preds)\n# Highest confidence per row\nhighest_probs = np.max(final_probs, axis=1)\nclass_names = le.classes_\n\n# ==========================================\n# SUBMISSION 1: ID + PREDICTED CLASS\n# ==========================================\nsubmission1_df = pd.DataFrame({\n    'id': test_id,\n    'fruit_name': decoded_labels\n})\n\n# ==========================================\n# SUBMISSION 2: ID + ALL CLASS PROBABILITIES log loss jaisa\n# ==========================================\nprob_cols = {\n    f\"Status_{cls}\": final_probs[:, i]\n    for i, cls in enumerate(class_names)\n}\n\nsubmission2_df = pd.DataFrame(prob_cols)\nsubmission2_df.insert(0, 'id', test_id)\n\n\n# ==========================================\n# SUBMISSION 3: ID + CLASS + CONFIDENCE random sa kuch to hai\n# ==========================================\nsubmission3_df = pd.DataFrame({\n    'id': test_id,\n    'Predicted_Class': decoded_labels,\n    'Confidence_Score': highest_probs\n})\n\n# ==========================================\n# STEP 11: EXPORT FILES 1- for prediction\n# 2- log loss styled\n# 3- just all the probablity in one line\n# ==========================================\n\nsubmission1_df.to_csv(\"submission1.csv\", index=False)\nsubmission2_df.to_csv(\"submission2.csv\", index=False)\nsubmission3_df.to_csv(\"submission3.csv\", index=False)\nprint(\"All submissions are generated\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# CONFUSION MATRIX (MULTI-CLASS)\n# ==========================================\ncm = confusion_matrix(y_test_enc, y_pred)\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(\n    cm,\n    annot=True,\n    fmt='d',\n    cmap='Blues',\n    xticklabels=le.classes_,\n    yticklabels=le.classes_\n)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}