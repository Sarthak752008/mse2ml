{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#For accuracy","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Imports\nimport numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler,OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import log_loss\n#train and test read\ntrain=pd.read_csv(\"/kaggle/input/mse2-makeup/train.csv\")\ntest=pd.read_csv(\"/kaggle/input/mse2-makeup/test.csv\")\n#some checks\ntrain.shape          # rows, columns\ntrain.columns        # column names\ntrain.head()         # first few rows\ntrain.tail()         # last few rows\ntrain.isnull().sum()\ntrain.info()\n#test item drop\ntest_id=test['id']\ntest=test.drop(columns=['id'])\n#train item drop\ntrain=train.drop(columns=['id'])\nX=train.drop(columns=['Status'])\ny=train['Status']\n#remove NaN from y\ntrain = train.dropna(subset=['Status'])#target\n#X and y drop\nX=train.drop(columns=[\"fruit_name\"])#target\ny=train[\"fruit_name\"]\n#train test split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n#features categories\nnumeric_features=X.select_dtypes(include=['int64','float64']).columns\ncategorical_features=X.select_dtypes(include=['object']).columns\n#feature pipeline\nnumerical_pipeline=Pipeline(steps=[\n    ('impute',SimpleImputer(strategy='mean')),\n    ('scaler',StandardScaler())\n])\ncategorical_pipeline=Pipeline(steps=[\n    ('impute',SimpleImputer(strategy='most_frequent')),\n    ('encode',OneHotEncoder(handle_unknown='ignore'))\n])\n#Pipeline for preprocessing\npreprocessing=ColumnTransformer(transformers=[\n    ('num',numerical_pipeline,numeric_features),\n    ('cat',categorical_pipeline,categorical_features)\n])\n#model selection\nmodel = GradientBoostingClassifier(\n    n_estimators=920,\n    learning_rate=0.02,\n    max_depth=4,\n    min_samples_split=2,\n    min_samples_leaf=1,\n    subsample=0.7,\n    random_state=42\n)\n#-----------------------------------------------\n#Grid search\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nmodel = GradientBoostingClassifier(random_state=42)\n\npipe = Pipeline(steps=[\n    ('preprocessing', preprocessing),\n    ('model', model)\n])\n\nparam_grid = {\n    'model__n_estimators': [400, 600, 800],\n    'model__learning_rate': [0.01, 0.03, 0.05],\n    'model__max_depth': [3, 4, 5],\n    'model__subsample': [0.6, 0.8, 1.0]\n}\n\ngrid = GridSearchCV(\n    estimator=pipe,\n    param_grid=param_grid,\n    scoring='neg_log_loss',\n    cv=5,\n    n_jobs=-1,\n    verbose=2\n)\n\ngrid.fit(X_train, y_train)\n\nprint(\"Best Log Loss:\", -grid.best_score_)\nprint(\"Best Params:\", grid.best_params_)\n#-----------------------------------------------\n#Randomised search\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom scipy.stats import randint, uniform\n\nmodel = GradientBoostingClassifier(random_state=42)\n\npipe = Pipeline(steps=[\n    ('preprocessing', preprocessing),\n    ('model', model)\n])\n\nparam_dist = {\n    'model__n_estimators': randint(300, 1200),\n    'model__learning_rate': uniform(0.01, 0.05),\n    'model__max_depth': randint(3, 6),\n    'model__subsample': uniform(0.6, 0.4)\n}\n\nrandom_search = RandomizedSearchCV(\n    estimator=pipe,\n    param_distributions=param_dist,\n    n_iter=30,\n    scoring='neg_log_loss',\n    cv=5,\n    random_state=42,\n    n_jobs=-1,\n    verbose=2\n)\n\nrandom_search.fit(X_train, y_train)\n\nprint(\"Best Log Loss:\", -random_search.best_score_)\nprint(\"Best Params:\", random_search.best_params_)\n\n#-----------------------------------------------\n#pipeline for model\npipeline=Pipeline(steps=[\n    ('preprocessor',preprocessing),\n    ('model',model)\n])\n#fitting the model\n#Agar grud search ker liya hai to grid.fit karna na ki pipeline.fit\npipeline.fit(X_train,y_train_enc)\n#predicting on X_test\ny_proba = pipeline.predict_proba(X_test)  # shape: (n_samples, n_classes)\n#predicting on test\ny_final=pipeline.predict_proba(test)\n#Submission\nsubmission = pd.DataFrame(\n    test_proba,\n    columns=class_names\n)\nsubmission.to_csv('submission.csv',index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#accuracy\nfrom sklearn.metrics import (\n    accuracy_score,\n    balanced_accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    confusion_matrix,\n    classification_report,\n    log_loss,\n    roc_auc_score,\n    top_k_accuracy_score,\n    matthews_corrcoef\n)\naccuracy_score(y_test, y_pred)\nf1_score(y_test, y_pred, average='macro')\nlog_loss(y_test, y_proba)\nconfusion_matrix(y_test, y_pred)\n\nfrom sklearn.metrics import (\n    mean_squared_error,\n    mean_absolute_error,\n    r2_score\n)\nrmse = mean_squared_error(y_test, y_pred, squared=False)\nmae  = mean_absolute_error(y_test, y_pred)\nr2   = r2_score(y_test, y_pred)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#plotting and visualization\n#A. DATA EXPLORATION (Before training)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12,5))\nsns.heatmap(train.isnull(), cbar=False)\nplt.title(\"Missing Values Heatmap\")\nplt.show()\n---------------------------------------------\ntrain['fruit_name'].value_counts().plot(\n    kind='bar', figsize=(8,4), title='Target Distribution'\n)\nplt.show()\n---------------------------------------------\ntrain.select_dtypes(include=['int64','float64']).hist(\n    figsize=(15,10), bins=30\n)\nplt.suptitle(\"Numeric Feature Distributions\")\nplt.show()\n----------------------------------------------\nplt.figure(figsize=(15,6))\nsns.boxplot(data=train.select_dtypes(include=['int64','float64']))\nplt.xticks(rotation=90)\nplt.title(\"Outlier Detection\")\nplt.show()\n----------------------------------------------\nplt.figure(figsize=(12,8))\nsns.heatmap(\n    train.corr(numeric_only=True),\n    cmap='coolwarm',\n    center=0\n)\nplt.title(\"Feature Correlation\")\nplt.show()\n----------------------------------------------\nplt.figure(figsize=(12,8))\nsns.heatmap(\n    train.corr(numeric_only=True),\n    cmap='coolwarm',\n    center=0\n)\nplt.title(\"Feature Correlation\")\nplt.show()\n----------------------------------------------\nAfter fitting\nfrom sklearn.metrics import confusion_matrix\nclasses = pipeline.named_steps['model'].classes_\n# y_pred = model.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(8,6))\nsns.heatmap(cm, annot=True, fmt='d',\n            xticklabels=classes,\n            yticklabels=classes)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n-----------------------------------------------\n# Scatter plot\nplt.figure(figsize=(6,4))\nsns.scatterplot(\n    x=train[num_cols_all[0]],\n    y=train[num_cols_all[1]],\n    hue=train[TARGET_COL],\n    legend=False\n)\nplt.title(\"Scatter Plot of Two Numeric Features\")\nplt.show()\n------------------------------------------------\nplt.figure(figsize=(10,6))\n\ncorr = train[num_cols_all[:10]].corr()\n\nsns.heatmap(\n    corr,\n    cmap=\"coolwarm\",\n    annot=True,\n    fmt=\".2f\"\n)\n\nplt.title(\"Correlation Heatmap (Numeric Features Only)\")\nplt.show()\n-------------------------------------------------\nConfusionMatrixDisplay.from_predictions(\n    y_val, y_pred_rf, display_labels=le.classes_, cmap=\"Blues\"\n)\nplt.title(\"RandomForest - Confusion Matrix\")\nplt.show()\n--------------------------------------------------\nplt.hist(y_proba.max(axis=1), bins=20)\nplt.xlabel(\"Max Predicted Probability\")\nplt.title(\"Prediction Confidence Distribution\")\nplt.show()\n--------------------------------------------------\n\n\nlog loss\n\n\nfrom sklearn.calibration import calibration_curve\n\nprob_true, prob_pred = calibration_curve(\n    (y_test == classes[0]).astype(int),\n    y_proba[:, 0],\n    n_bins=10\n)\n\nplt.plot(prob_pred, prob_true, marker='o')\nplt.plot([0,1], [0,1], linestyle='--')\nplt.xlabel(\"Predicted Probability\")\nplt.ylabel(\"True Probability\")\nplt.title(\"Calibration Curve (Class 0)\")\nplt.show()\n---------------------------------------------------\nplt.figure(figsize=(6,6))\nplt.plot(prob_pred, prob_true, label='Before')\nplt.plot([0,1],[0,1],'--')\nplt.legend()\nplt.title(\"Calibration Comparison\")\nplt.show()\n---------------------------------------------------\n#submission quality\n\n\nplt.hist(submission.drop(columns=['id']).sum(axis=1))\nplt.title(\"Probability Sum per Row (Should be ~1)\")\nplt.show()\n\n\nsubmission.drop(columns=['id']).mean().plot(\n    kind='bar', title=\"Mean Predicted Probabilities\"\n)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#For Log loss or neg_log_los","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Imports\nimport numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler,OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import log_loss\n#train and test read\ntrain=pd.read_csv(\"/kaggle/input/mock-test-2-mse-2/train.csv\")\ntest=pd.read_csv(\"/kaggle/input/mock-test-2-mse-2/test.csv\")\n#some checks\ntrain.shape          # rows, columns\ntrain.columns        # column names\ntrain.head()         # first few rows\ntrain.tail()         # last few rows\ntrain.isnull().sum()\ntrain.info()\n#test item drop\ntest_id=test['id']\ntest=test.drop(columns=['id'])\n#train item drop\ntrain=train.drop(columns=['id'])\nX=train.drop(columns=['Status'])\ny=train['Status']\n#remove NaN from y\ntrain = train.dropna(subset=['fruit_name'])\n#train test split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n#features categories\nnumeric_features=X.select_dtypes(include=['int64','float64']).columns\ncategorical_features=X.select_dtypes(include=['object']).columns\n#feature pipeline\nnumerical_pipeline=Pipeline(steps=[\n    ('impute',SimpleImputer(strategy='mean')),\n    ('scaler',StandardScaler())\n])\ncategorical_pipeline=Pipeline(steps=[\n    ('impute',SimpleImputer(strategy='most_frequent')),\n    ('encode',OneHotEncoder(handle_unknown='ignore'))\n])\n#Pipeline for preprocessing\npreprocessing=ColumnTransformer(transformers=[\n    ('num',numerical_pipeline,numeric_features),\n    ('cat',categorical_pipeline,categorical_features)\n])\n#model selection\nmodel = GradientBoostingClassifier(\n    n_estimators=920,\n    learning_rate=0.02,\n    max_depth=4,\n    min_samples_split=2,\n    min_samples_leaf=1,\n    subsample=0.7,\n    random_state=42\n)\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nmodel = HistGradientBoostingClassifier(\n    max_iter=600,\n    learning_rate=0.03,\n    max_depth=6,\n    min_samples_leaf=20,\n    l2_regularization=0.1,\n    max_bins=255,\n    random_state=42\n)\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nmodel = ExtraTreesClassifier(\n    n_estimators=900,\n    max_depth=None,\n    min_samples_split=2,\n    min_samples_leaf=1,\n    max_features='sqrt',\n    bootstrap=False,\n    random_state=42,\n    n_jobs=-1\n)\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(\n    n_estimators=800,\n    max_depth=None,\n    min_samples_split=2,\n    min_samples_leaf=1,\n    max_features='sqrt',\n    bootstrap=True,\n    random_state=42,\n    n_jobs=-1\n)\nfrom sklearn.ensemble import AdaBoostClassifier\n\nmodel = AdaBoostClassifier(\n    n_estimators=600,\n    learning_rate=0.03,\n    algorithm='SAMME.R',\n    random_state=42\n)\nfrom xgboost import XGBClassifier\n\nmodel = XGBClassifier(\n    n_estimators=800,\n    learning_rate=0.03,\n    max_depth=6,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    objective='multi:softprob',\n    eval_metric='mlogloss',\n    random_state=42,\n    n_jobs=-1\n)\nfrom lightgbm import LGBMClassifier\n\nmodel = LGBMClassifier(\n    n_estimators=900,\n    learning_rate=0.03,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    objective='multiclass',\n    random_state=42,\n    n_jobs=-1\n)\nfrom catboost import CatBoostClassifier\n\nmodel = CatBoostClassifier(\n    iterations=900,\n    learning_rate=0.03,\n    depth=6,\n    loss_function='MultiClass',\n    eval_metric='MultiClass',\n    random_seed=42,\n    verbose=False\n)\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(\n    multi_class='multinomial',\n    solver='lbfgs',\n    C=1.0,\n    max_iter=2000,\n    n_jobs=-1\n)\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier(\n    n_neighbors=15,\n    weights='distance',\n    metric='minkowski'\n)\n\n#pipeline for model\npipeline=Pipeline(steps=[\n    ('preprocessor',preprocessing),\n    ('model',model)\n])\n#label encoder\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ny_train_enc = le.fit_transform(y_train)  # fit on train\ny_test_enc = le.transform(y_test)        # transform test\n#fitting the model\npipeline.fit(X_train,y_train_enc)\n#predicting on X_test\ny_proba = pipeline.predict_proba(X_test)  # shape: (n_samples, n_classes)\n#predicting on test\ny_final=pipeline.predict_proba(test)\n#re labeleing and submission\nclass_names = le.classes_  # use label encoder mapping\nsubmission = pd.DataFrame(y_final, columns=[f\"Status_{cls}\" for cls in class_names])\nsubmission.insert(0, 'id', test_id)\nsubmission.to_csv(\"submission4.csv\", index=False)\nprint(\"\\nâœ… Submission file created successfully!\")\nprint(submission.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}